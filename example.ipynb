{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Log Analysis Examples with pandas\n",
      "\n",
      "This notebook contains many useful examples of analysis that can be done on a log file once it has been parsed.  This includes graphing\n",
      "and histogramming garbage collections, compactions, flushes, and exception frequency, as well as querying version and environmental information.\n",
      "\n",
      "Refer to [Diving into Open Data with IPython Notebook & Pandas](http://nbviewer.ipython.org/github/jvns/talks/blob/master/pyconca2013/pistes-cyclables.ipynb) for a quick overview of using IPython and pandas together.  More information can be found on the [IPython](http://ipython.org/) and [pandas](http://pandas.pydata.org/) web pages.\n",
      "\n",
      "These are just some of the things you could do. IPython makes exploratory computing easy, so don't hesitate to try new things!\n",
      "\n",
      "To run a block, click on the code and press `Ctrl`+`Enter`.  You must do this for each individual block in order to refresh it after loading a new log."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Initialization\n",
      "\n",
      "**Important!** These two blocks must be run before you can do any of the examples below. Note that not every example will work with every log. It depends on what's in the log.\n",
      "\n",
      "- Block #1 imports all of the libraries and does any configuration necessary.\n",
      "- Block #2 parses the log file. There are 3 example log files included, so try them all. You can also use your own log file by passing its full path to the SystemLog constructor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab\n",
      "import pandas as pd\n",
      "import pprint\n",
      "import cassandra\n",
      "reload(cassandra)\n",
      "\n",
      "pylab.rcParams['figure.figsize'] = 18, 12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logfiles = ['system0.log', 'system1.log', 'system2.log']\n",
      "\n",
      "#0=Good general example\n",
      "#1=Good compaction example\n",
      "#2=Good GC Example\n",
      "\n",
      "log = cassandra.SystemLog(logfiles[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Environment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log.sessions[1]['environment']['jvm']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_records(log.sessions[1]['versions'], columns=['component','version'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Garbage Collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gc = pd.DataFrame.from_records(log.sessions[0]['garbage_collections'], \n",
      "                               columns=['date', 'collections', 'duration'], index='date')\n",
      "#day=gc['20131012']\n",
      "hourly = gc.resample('H', how='sum')\n",
      "hourly.plot(y='duration', logy=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gc['duration'].hist(bins=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_records(log.sessions[0]['heap_warnings'], index='date')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Compactions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compactions = pd.DataFrame.from_records(log.sessions[0]['compactions'],index='begin_date')\n",
      "compactions.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compactions = compactions.dropna()\n",
      "compactions['rate'].resample('H', how='count').plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compactions['rate'].hist(bins=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Flushes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flushes = pd.DataFrame.from_records(log.sessions[0]['flushes'], index='begin_date')\n",
      "flushes['serialized_bytes'].resample('H', how='count').plot()\n",
      "#flushes['queue_duration'] = flushes['begin_date'] - flushes['enqueue_date']\n",
      "#flushes['flush_duration'] = flushes['end_date'] - flushes['begin_date']\n",
      "#flushes.sort('flush_duration')\n",
      "#flushes.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errors = pd.DataFrame.from_records(log.sessions[0]['errors'], index='date')\n",
      "errors['exception'].value_counts().plot(kind='barh')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Repairs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp = []\n",
      "for session in log.sessions[6]['repair_sessions']:\n",
      "    tmp.extend(session.get('inconsistent_endpoints', []))\n",
      "inconsistent = pd.DataFrame.from_records(tmp, index='date')\n",
      "inconsistent.sort(columns=['session_id', 'column_family', 'node1', 'node2']).tail(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp = []\n",
      "for session in log.sessions[6]['repair_sessions']:\n",
      "    tmp.extend(session.get('merkle_requests', []))\n",
      "merkle_requests = pd.DataFrame.from_records(tmp, index='request_date')\n",
      "merkle_requests.tail(60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "streaming_sessions = pd.DataFrame.from_records(log.sessions[-1]['streaming_sessions'])\n",
      "streaming_sessions.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sstables_sent = pd.DataFrame.from_records(log.sessions[6]['sstables_sent'], index='date')\n",
      "sstables_sent.tail(60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Status Logger"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thread_pool = pd.DataFrame.from_records(log.sessions[0]['status'][10]['thread_pool'],index='pool_name')\n",
      "thread_pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "caches = pd.DataFrame.from_records(log.sessions[0]['status'][2]['caches'],index='type')\n",
      "caches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "memtables = pd.DataFrame.from_records(log.sessions[0]['status'][2]['memtables'], columns=['keyspace','column_family','ops','data'])\n",
      "memtables.tail(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Unrecognized Messages\n",
      "\n",
      "The log parser isn't complete and probably will always be a work in progress. The goal is to parse the most useful messages first.  This report shows the number of unrecognized messages for each category/source file.  Generally when writing a new parser, I choose one of the categories at the top of the list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unknown = pd.DataFrame.from_records(log.unknown_messages, index='date')\n",
      "unknown[unknown.level == 'INFO'].source_file.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Solr Indexing\n",
      "\n",
      "This will index the events from the currently loaded log into the Solr schema defined in `schema.xml` included in the logparse directory, where they can be searched.  You can use the `create-schema.sh` script in the same directory to create the schema in DSE. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log.solr_index('http://localhost:8983/solr/systemlog.log')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Elastic Search Indexing\n",
      "\n",
      "This will index the log events into Elastic Search so they can be explored using Kibana. Tested with Elastic Search 1.0 and Kibana 3.0, no special configuration needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log.elastic_index('http://localhost:9200/systemlog/line/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}